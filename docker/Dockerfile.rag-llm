# Base: official vLLM OpenAI-compatible image
FROM vllm/vllm-openai:latest

# Workdir (mainly for future extensions)
WORKDIR /workspace

# Default model/config (can be overridden at runtime with CLI args)
ENV MODEL_NAME="Qwen/Qwen2.5-3B-Instruct" \
    MAX_MODEL_LEN="8192" \
    GPU_MEMORY_UTILIZATION="0.90" \
    MAX_NUM_SEQS="64" \
    PORT="8000"

# Expose API port
EXPOSE 8000

# vLLM image already has: ENTRYPOINT ["vllm", "serve"]
ENTRYPOINT ["vllm", "serve"]
# We just provide default arguments via CMD in exec form (no shell munging).
CMD [
  "Qwen/Qwen2.5-3B-Instruct",
  "--host", "0.0.0.0",
  "--port", "8000",
  "--max-model-len", "8192",
  "--gpu-memory-utilization", "0.90",
  "--max-num-seqs", "64"
]
